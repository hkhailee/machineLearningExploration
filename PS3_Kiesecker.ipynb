{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS3_Kiesecker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rziuZ2tHIsdI"
      },
      "source": [
        "# Hailee Kiesecker\n",
        "# Colab : None\n",
        "# PS3 ML Spring 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izy3rcdBIqhP"
      },
      "source": [
        "# Question 1 (15 pts) (Scikit Allowed) SVM\n",
        "\n",
        "To classify iris dataset (Iris-Versicolor vs. others) in the best way, you have to create an algorithm able to\n",
        "determine (with the k-fold cross validation) what is the best space transformation (among rbf_kernel, polynomial\n",
        "features, and polynomial kernel) and its hyperparameters. Each transformation has its own parameters\n",
        "rbf_kernel->gamma, polynomial-features-> degree, polynomial_kernel ->(gamma, degree). The performance\n",
        "must be tested of the entire algorithm with another K-fold cross-validation (please use then trainset, validationset,\n",
        "and testset).\n",
        "Hint: use 3 k-folds.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmrWRmKpJlTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25576af-463e-4ae8-daa6-b058c5e46cf8"
      },
      "source": [
        "# imports \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics.pairwise import polynomial_kernel\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import svm\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfBZ4r_KJmtT"
      },
      "source": [
        "# set up data \n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit_transform(X)\n",
        "# make y into either Versicolor or not\n",
        "y = np.where(y == 1, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53FI5jbpICbi"
      },
      "source": [
        "# keeping out a test set (X/y_val) for final model evaluation\n",
        "X, X_val, y, y_val = train_test_split(X, y, test_size=0.2, random_state=0) #holding out 20% of data for final model \n",
        "\n",
        "kf = KFold(n_splits=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu2ReTa7dy9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e27d857-08af-45a9-f972-0f29ce2d90e6"
      },
      "source": [
        "#Display later\n",
        "#===================================\n",
        "# best poly kernel based on F1\n",
        "bestF1_plyK = 0\n",
        "bestModel_plyK = None\n",
        "\n",
        "# best rbf kernel based on F1\n",
        "bestF1_rbf = 0\n",
        "bestModel_rbf = None\n",
        "\n",
        "#best poly feature based on F1\n",
        "bestF1_plyF = 0\n",
        "bestModel_plyF = None\n",
        "#===================================\n",
        "\n",
        "#we have data splitting into 3 parts ===========================================\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  # find the best hyper parameter of the fold\n",
        "\n",
        "  #poly kernel -- gamma and degree----------------------------------------------\n",
        "  plyK_param = {'degree': [1,7,14,27,34],\n",
        "                'C':[0.01, 0.15, 0.2, 0.25, 0.4],  # changed gamma to C\n",
        "                'kernel': ['poly']\n",
        "                }\n",
        "  plyK_grid = GridSearchCV(svm.SVC(), plyK_param)\n",
        "  plyK_grid.fit(X_train, y_train) # we now have best hyper-parameters for this fold\n",
        "  print(\"Best poly Kernel: {0}, using {1}\".format(plyK_grid.best_score_, plyK_grid.best_params_))\n",
        "\n",
        "  #rbf kernel - just gamma------------------------------------------------------\n",
        "  rbf_param = {'gamma': [0.1, 0.2, 0.4, 6.0, 10],\n",
        "                'C':[1,10,100],\n",
        "                'kernel': ['rbf'] }\n",
        "  rbf_grid = GridSearchCV(svm.SVC(), rbf_param)\n",
        "  rbf_grid.fit(X_train, y_train) # we now have best hyper-parameters for this fold\n",
        "  print(\"Best rbf Kernel: {0}, using {1}\".format(rbf_grid.best_score_, rbf_grid.best_params_))\n",
        "\n",
        "  #poly feature - just degree ( not sure if this is working)--------------------\n",
        "  plyF_param = {'degree': [1,2,3,4,5,6,7,8,9,10,14,27,34] }\n",
        "  plyF_grid = GridSearchCV(PolynomialFeatures(),\n",
        "                         plyF_param, \n",
        "                         scoring='neg_mean_squared_error') \n",
        "  #X_poly = plyF_grid.fit(X_train, y_train)\n",
        "\n",
        "  #best hyper parameter F1 score ===============================================\n",
        "  for a, b in kf.split(X_train):\n",
        "    X_train_splitsplit, X_test_splitsplit = X[a], X[b]\n",
        "    y_train_splitsplit, y_test_splitsplit = y[a], y[b]\n",
        "\n",
        "    #poly kernel testing -------------------------------------------------------\n",
        "    clf_plyK = svm.SVC(**plyK_grid.best_params_)\n",
        "    clf_plyK.fit(X_train_splitsplit, y_train_splitsplit)\n",
        "    y_pred_plyK = clf_plyK.predict(X_test_splitsplit)\n",
        "    f1_plyK = f1_score(y_test_splitsplit, y_pred_plyK, average='macro')\n",
        "    print(\"Best poly Kernel F1 score: {0}, using {1}\".format(f1_plyK, plyK_grid.best_params_))\n",
        "    # storing best values for poly kernel\n",
        "    if bestF1_plyK < f1_plyK:\n",
        "      bestF1_plyK = f1_plyK\n",
        "      bestModel_plyK = plyK_grid.best_params_\n",
        "\n",
        "    #rbf kernel ----------------------------------------------------------------\n",
        "    clf_rbf = svm.SVC(**rbf_grid.best_params_)\n",
        "    clf_rbf.fit(X_train_splitsplit, y_train_splitsplit)\n",
        "    y_pred_rbf = clf_rbf.predict(X_test_splitsplit)\n",
        "    f1_rbf = f1_score(y_test_splitsplit, y_pred_rbf, average='macro')\n",
        "    print(\"Best rbf Kernel F1 score: {0}, using {1}\".format(f1_rbf, rbf_grid.best_params_))\n",
        "    # storing best values for rbf kernel\n",
        "    if bestF1_rbf < f1_rbf:\n",
        "      bestF1_rbf = f1_rbf\n",
        "      bestModel_rbf = rbf_grid.best_params_\n",
        "\n",
        "    #poly Feature (not sure if this is working) --------------------------------\n",
        "    #clf_plyF= X_poly.fit(X_train_splitsplit, y_train_splitsplit)\n",
        "    #predict =clf_plyF.fit_transform(X_test_splitsplit)\n",
        "    #clf = linear_model.LinearRegression()\n",
        "    #clf.fit(X_poly, predict)\n",
        "    #y_pred_plyK = clf.predict(predict)\n",
        "    #F1_plyF = f1_score(y_test_splitsplit, y_pred_plyK, average='macro')\n",
        "    # storing best values for rbf kernel\n",
        "    #if bestF1_plyF < F1_plyF:\n",
        "      #bestF1_plyF = F1_plyF\n",
        "      #bestModel_plyF = plyF_grid.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best poly Kernel: 0.975, using {'C': 0.01, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel: 0.9875, using {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 0.8712241653418125, using {'C': 0.01, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 0.9570747217806042, using {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 0.961038961038961, using {'C': 0.01, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 0.9205882352941177, using {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 0.9174603174603174, using {'C': 0.01, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 1.0, using {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Best poly Kernel: 0.9125, using {'C': 0.4, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel: 0.925, using {'C': 1, 'gamma': 0.2, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 0.8712241653418125, using {'C': 0.4, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 0.9570747217806042, using {'C': 1, 'gamma': 0.2, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 1.0, using {'C': 0.4, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 0.9205882352941177, using {'C': 1, 'gamma': 0.2, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 0.9174603174603174, using {'C': 0.4, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 1.0, using {'C': 1, 'gamma': 0.2, 'kernel': 'rbf'}\n",
            "Best poly Kernel: 0.95, using {'C': 0.25, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel: 0.95, using {'C': 1, 'gamma': 0.4, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 0.8712241653418125, using {'C': 0.25, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 0.9166666666666666, using {'C': 1, 'gamma': 0.4, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 1.0, using {'C': 0.25, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 0.8781954887218045, using {'C': 1, 'gamma': 0.4, 'kernel': 'rbf'}\n",
            "Best poly Kernel F1 score: 1.0, using {'C': 0.25, 'degree': 7, 'kernel': 'poly'}\n",
            "Best rbf Kernel F1 score: 1.0, using {'C': 1, 'gamma': 0.4, 'kernel': 'rbf'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTlIWAOWsf7v"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "avgRecall_rbf =0 \n",
        "avgRecall_plyK =0 \n",
        "avgPre_rbf =0 \n",
        "avgPre_plyK =0\n",
        "avgF1_rbf =0 \n",
        "avgF1_plyK =0  \n",
        "#test on validation set\n",
        "for train_index, test_index in kf.split(X_val):\n",
        "  X_train, X_test = X_val[train_index], X_val[test_index]\n",
        "  y_train, y_test = y_val[train_index], y_val[test_index]\n",
        "\n",
        "  clf_polyKernel = svm.SVC(**bestModel_plyK)\n",
        "  clf_rbf = svm.SVC(**bestModel_rbf)\n",
        "\n",
        "  clf_polyKernel.fit(X_train, y_train)\n",
        "  clf_rbf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred_plyK = clf_polyKernel.predict(X_test)\n",
        "  y_pred_rbf = clf_rbf.predict(X_test)\n",
        "\n",
        "  avgRecall_rbf = avgRecall_rbf + recall_score(y_test, y_pred_rbf, average='micro')\n",
        "  avgRecall_plyK = avgRecall_plyK + recall_score(y_test, y_pred_plyK, average='micro')\n",
        "\n",
        "  avgPre_rbf = avgPre_rbf + precision_score(y_test, y_pred_rbf, average='micro')\n",
        "  avgPre_plyK = avgPre_plyK + precision_score(y_test, y_pred_plyK, average='micro')\n",
        "\n",
        "  avgF1_rbf = avgF1_rbf + f1_score(y_test, y_pred_rbf, average='micro')\n",
        "  avgF1_plyK = avgF1_plyK + f1_score(y_test, y_pred_plyK, average='micro')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtHX38tsJ8lJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12efb66f-2549-4bae-f566-069c1201d701"
      },
      "source": [
        "#avg metrics for rbf kernel\n",
        "print(\"rbf kernel  ; avg recall\",avgRecall_rbf/3, \"avg precsion\" ,avgPre_rbf/3, \"avg F1\",avgF1_rbf/3 , \"using:\" ,bestModel_rbf)\n",
        "#avg metrics for poly kernel\n",
        "print(\"poly kernel ; avg recall\",avgRecall_plyK/3, \"avg precsion\" ,avgPre_plyK/3, \"avg F1\",avgF1_plyK/3, \"using:\", bestModel_plyK)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rbf kernel  ; avg recall 0.9666666666666667 avg precsion 0.9666666666666667 avg F1 0.9666666666666667 using: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "poly kernel ; avg recall 0.8666666666666666 avg precsion 0.8666666666666666 avg F1 0.8666666666666666 using: {'C': 0.4, 'degree': 7, 'kernel': 'poly'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLC_1WFKEDG2"
      },
      "source": [
        "# Question 1 : Reflection\n",
        "From our current implementation it appears the best space transformation amoung rbf and poly kernel is RBF with hyper perameters being C: 10 , Gamma 0.1\n",
        "\n",
        "Current testing on polynomial features is not working, if time permits going back and studying how to implement polynomial features fully will be done. Currently it is implemented but in a way that causes errors for the overall structure and will continue to be commented out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AradvvrbK1p"
      },
      "source": [
        "# Question 2\n",
        "Write the explicit constraints (without using any vectorial notation, as a summation of single variables multiplied\n",
        "by a constant + bias term) of the Support Vector Machine to correctly classify iris dataset (Iris-Versicolor vs.\n",
        "others). Use 5 points in Iris-Versicolor, 2 points for iris-setosa, and 3 points for iris Virginia.\n",
        "Please show the points you selected and after the constraints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu2ROx1hCN4n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "92da895c-b547-4b46-90f9-70bdb10634bc"
      },
      "source": [
        "#for a data point [0.2,0.2,1.5,6.8, '0'] -> \\sum(0.2*W1 + 0.2*W2+1.5*W3+6.8*W4) + b < -1\n",
        "# set up data \n",
        "iris = pd.read_csv(\"/iris.csv\")\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "y = np.where(y == 1, 1, 0)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit_transform(X)\n",
        "# getting weights\n",
        "test = svm.SVC(kernel='linear')\n",
        "test.fit(X, y)\n",
        "#print(test.coef_)\n",
        "\n",
        "#==========================================================================\n",
        "# when 1 vs 1\n",
        "#[-0.10016464 -2.10358464  0.61654795 -1.45218908]\n",
        "#==========================================================================\n",
        "# when 1 vs 2 \n",
        "#[-0.04625854  0.5211828  -1.00304462 -0.46412978] # 0Iris-setosa\n",
        "# [-0.00722313  0.17894121 -0.53836459 -0.29239263] # 1Iris-versicolor\n",
        "# [ 0.59549776  0.9739003  -2.03099958 -2.00630267] # 2Iris-virginica\n",
        "#==========================================================================\n",
        "\n",
        "w1 = -0.10016464\n",
        "w2 = -2.10358464\n",
        "w3 = 0.61654795\n",
        "w4 = -1.45218908\n",
        "b = None # find bias\n",
        "\n",
        "# equation 1 point 1 Iris-versicolor---------------------\t\n",
        "ver1 = (5.7*w1+\t2.8*w2+\t4.1*w3+\t1.3*w4) \n",
        "# point 2 \n",
        "ver2 = (6.2*w1+\t2.9*w2+\t4.3*w3+\t1.3*w4)\n",
        "#point 3 \n",
        "ver3 = (5.6*w1+\t3*w2+\t4.1*w3+\t1.3*w4) \n",
        "# point 4\n",
        "ver4 = (6*w1+\t2.7*w2+\t5.1*w3+\t1.6*w4) \n",
        "#point 5 \n",
        "ver5 = (5.6*w1+ 2.5*w2+\t3.9*w3+\t1.1*w4)\n",
        "\n",
        "# equation 1 point 1 \tIris-virginica---------------------\t\t\t\n",
        "vir1 = (6.8*w1+\t3.2*w2+\t5.9*w3+\t2.3*w4)\n",
        "#point 2\n",
        "vir2 = (6.3*w1+\t2.7*w2+\t4.9*w3+\t1.8*w4)\n",
        "#point 3 \n",
        "vir3 = (6.4*w1+\t3.1*w2+\t5.5*w3+\t1.8*w4)\t\t\n",
        "\n",
        "#equation 1 point 1 Iris-setosa--------------------------\n",
        "set1 = (4.6*w1+\t3.2*w2+\t1.4*w3+\t0.2*w4)\t\n",
        "#point 2\n",
        "set2 = (4.3*w1+\t3*w2+\t1.1*w3+\t0.1*w4)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "\n",
        "# function to find the bias that satifies the constraint _ver\n",
        "done = False\n",
        "def findBias_Ver(equation, bias):\n",
        "  while (done == False):\n",
        "    if (equation+bias <-1):\n",
        "      break\n",
        "    else:\n",
        "      bias = bias - 0.01\n",
        "  return bias\n",
        "# function to find the bias that satifies the constraint _rest\n",
        "done1 = False\n",
        "def findBias_rest(equation, bias):\n",
        "  while (done1 == False):\n",
        "    if (equation+bias>=-1 and equation+bias <= 1):\n",
        "      break\n",
        "    else:\n",
        "      bias = bias - 0.01\n",
        "  return bias\n",
        "\t\t\n",
        "#calc of bias's\n",
        "b1 = (findBias_Ver(ver1, 1000))\n",
        "b2 = (findBias_Ver(ver2, 1000))\n",
        "b3= (findBias_Ver(ver3, 1000))\n",
        "b4 = (findBias_Ver(ver4, 1000))\n",
        "b5 = (findBias_Ver(ver5, 1000))\n",
        "b1_rest = (findBias_rest(vir1, 1000))\n",
        "b2_rest = (findBias_rest(vir2, 1000))\n",
        "b3_rest = (findBias_rest(vir3, 1000))\n",
        "b4_rest = (findBias_rest(set1, 1000))\n",
        "b5_rest = (findBias_rest(set2, 1000))\n",
        "# average bias of each class\n",
        "bias_ver = (b1+b2+b3+b4+b5) / 5\n",
        "bias_rest = (b1_rest+b2_rest+b3_rest+b4_rest+b5_rest)/5\n",
        "\n",
        "print(\"bias for ver\" ,bias_ver)\n",
        "print(\"bias for rest\",bias_rest)\n",
        "# point creation \n",
        "p1 = ver1+bias_ver\n",
        "p2 = ver2+bias_ver\n",
        "p3 = ver3+bias_ver\n",
        "p4 = ver4+bias_ver\n",
        "p5 = ver5+bias_ver\n",
        "p1r = set1+bias_ver\n",
        "p2r = set2+bias_ver\n",
        "p3r = vir1+bias_ver\n",
        "p4r = vir2+bias_ver\n",
        "p5r = vir3+bias_ver\n",
        "\n",
        "#plotting of chosen points\n",
        "data = ([p1,\"ver\"],[p2,\"ver\"],[p3,\"ver\"],[p4,\"ver\"],[p5,\"ver\"],[p1r,\"rest\"],[p2r,\"rest\"],[p3r,\"rest\"],[p4r,\"rest\"],[p5r,\"rest\"])\n",
        "df = pd.DataFrame(data ,columns = ['numeric','type'])\n",
        "df['type']=df['type'].astype('|S80')\n",
        "#print(df.info())\n",
        "sns.scatterplot(data=df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bias for ver 4.692000000764443\n",
            "bias for rest 7.440000000764385\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   numeric  10 non-null     float64\n",
            " 1   type     10 non-null     |S80   \n",
            "dtypes: bytes640(1), float64(1)\n",
            "memory usage: 1008.0 bytes\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fad7b7a9a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY92TAEIqgs8"
      },
      "source": [
        "# Question 2 Reflection \n",
        "\n",
        "Overall I had a hard time with this part of the assignment. I could watch videos on SVM and read powerpoints on how they worked but when it came down to implementing the math I didnt fully understand how everything tied together. I tried to implement what I thought was the correct way by getting the weights from each feature, but mathmatically I didnt fully understand how to do that so I just used a built in function of SVM. I also wasnt sure if I should go through every single data entry and make points and then choose the most optimal points, or if random was alright, so I just went with random.\n",
        "\n",
        "While I do not believe I did this a fully correct way our end points are seperated into two groups in the displayed scatter plot. Saddly I could not get it to color these dots into different Hues, probably a simple error on my end in creation of the data... actually I am going to try a simple fix bellow: ---\n",
        "\n",
        "The Quick fix ended up also having the same simple error. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxe0g8MCjdGa"
      },
      "source": [
        "#data = ([ver1,b1,\"ver\"],[ver2,b2,\"ver\"],[ver3,b3,\"ver\"],[ver4,b4,\"ver\"],[ver5,b5,\"ver\"],[vir1,b1_rest,\"rest\"],[vir2,b2_rest,\"rest\"],[vir3,b3_rest,\"rest\"],[set1,b4_rest,\"rest\"],[set2,b5_rest,\"rest\"])\n",
        "##df = pd.DataFrame(data ,columns = ['x','y','type'])\n",
        "##sns.scatterplot(data=df, hue = 'type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W322Pr7rt3OE"
      },
      "source": [
        "# Extra credit (5 pts)\n",
        "An unbalanced dataset (e.g. 95% vs 5%) can be problematic even in the training phase. The learned function can\n",
        "be trivial, e.g. always predicting one class. A possible solution can have a weight for each point in the way that\n",
        "making a mistake in the minority class will count more w.r.t. the other. Please redefine the likelihood of the\n",
        "logistic regression to consider these weights for each point. Please compute the log-likelihood and its derivatives.\n",
        "In addition, add to the negative log-likelihood the norm of W (sum of the square of each component) and compute\n",
        "the derivatives. You should not expect class_weight parameters and SMOTE to give the exact same results\n",
        "because they are different methods.\n",
        "Class weights directly modify the loss function by giving more (or less) penalty to the classes with more (or less)\n",
        "weight. In effect, one is basically sacrificing some ability to predict the lower weight class (the majority class\n",
        "for unbalanced datasets) by purposely biasing the model to favor more accurate predictions of the higher\n",
        "weighted class (the minority class).\n",
        "Oversampling and undersampling methods essentially give more weight to particular classes as well (duplicating\n",
        "observations duplicates the penalty for those particular observations, giving them more influence in the model\n",
        "fit), but due to data splitting that typically takes place in training this will yield slightly different results as well.\n",
        "SMOTE creates new observations of the minority class by randomly sampling from a set of \"similar\" minority\n",
        "class observations. Synthesized observations are computed based on adding a random percentage of the\n",
        "difference between two randomly chosen \"similar\" observations for each coordinate (i.e. column). Similar\n",
        "observations are defined typically using the k most closest neighbors to a particular observation of the minority\n",
        "class. This means that, depending on the value of k chosen, as well as numerous other factors such as how similar\n",
        "your observations are in general, the distance measure, etc. SMOTE may or may not be useful for your problem.\n",
        "Methods that deal with class imbalance do not all work the same and is a large area of study. It appears you have\n",
        "noticed that the class weight method is more effective, which is why any sort of method chosen in addressing\n",
        "any potential class imbalance problem needs to be wrapped within a model validation scheme to see if the\n",
        "particular method (or really, using any method at all) yields any benefit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUpdy1eAt9pY"
      },
      "source": [
        "# The Basic Question\n",
        "Please redefine the likelihood of the\n",
        "logistic regression to consider these weights for each point. Please compute the log-likelihood and its derivatives.\n",
        "In addition, add to the negative log-likelihood the norm of W (sum of the square of each component) and compute\n",
        "the derivatives. You should not expect class_weight parameters and SMOTE to give the exact same results\n",
        "because they are different methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCOj0rV1rjXP"
      },
      "source": [
        "#Function to normalize the weights of a given set of features using loglikelyhood, derv, Logistic regression\n",
        "def LogLike_wDIR(weight):\n",
        "\n",
        "  #takes in a weight \n",
        "\n",
        "  # computes Log likelyhood\n",
        "\n",
        "  # takes derivitives \n",
        "\n",
        " return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyRNqxPSufqe"
      },
      "source": [
        "#Function to Modify the weights with normalization-ish\n",
        "def SumSquare (W):\n",
        "  #add Negate of Log Like\n",
        "\n",
        "  # compute Derivitives \n",
        "  return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vflNnmnFvEu2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}