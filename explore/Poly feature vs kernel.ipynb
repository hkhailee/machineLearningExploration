{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q6_Kiesecker-C.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMgY5G9hnAClMPzWYKvOSjX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uX7plJYDrL9f"},"source":["# Question 6 (10 pts) -- COMPLETE\n","Show examples or situations when the use of kernel procedure is more efficient in terms of training and prediction computational time w.r.t. polynomial features transformation\n","\n","interpeitation of that:\n","\n","Sometimes data is not clearly seperatable. To be able to seperate it you need to manipulate the features to make a fairly ok seperation line. To do this you can always transform the features but if you transform each point, you can run into huge computational times. \n","\n","To avoid that issue you can use a kernel procedure such as gaussian kernel to speed up the process and allow for more complex categorizations."]},{"cell_type":"code","metadata":{"id":"G2v62qheAMeU","executionInfo":{"status":"ok","timestamp":1618015518665,"user_tz":360,"elapsed":418,"user":{"displayName":"Hailee K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-K-gnA0SyJcEz2RHEd55BjXelge052dstnK98aA=s64","userId":"07223577005008820598"}}},"source":["# lets get some play data \n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np \n","import time\n","\n","from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.metrics import precision_score, accuracy_score\n","\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","\n","iris = datasets.load_iris()\n","\n","iris_X = iris.data\n","\n","iris_y = iris.target\n","\n","\n","X = pd.DataFrame(iris_X, columns=iris.feature_names)\n","y = pd.DataFrame(iris_y, columns={\"target\"})\n","\n","# split\n","X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size = 0.4)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jj14HrYF12t"},"source":["Basic implemenation with polynomial feature transformation using logistic regression classifier --NO KERNEL"]},{"cell_type":"code","metadata":{"id":"xJC8Airf7LTW","executionInfo":{"status":"ok","timestamp":1618014909005,"user_tz":360,"elapsed":745,"user":{"displayName":"Hailee K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-K-gnA0SyJcEz2RHEd55BjXelge052dstnK98aA=s64","userId":"07223577005008820598"}}},"source":["def poly_linear_base(Xtrain, ytrain, Xtest, ytest):\n","    tic = time.perf_counter()\n","    train_x = Xtrain\n","    train_y = ytrain\n","    test_x = Xtest\n","    test_y = ytest\n","    reg_pipe = Pipeline([('standardize', StandardScaler()),\n","                         ('polynomial', PolynomialFeatures(degree=4, include_bias=False)),\n","                         ('classify', LogisticRegression())])\n","\n","    reg_pipe.fit(train_x, train_y)\n","    ac = accuracy_score(test_y, reg_pipe.predict(test_x))\n","\n","    toc = time.perf_counter()\n","    print(f\"standard logic regression in {toc - tic:0.4f} seconds with accuracy of {ac}\")\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JK9jm8257MDs","executionInfo":{"status":"ok","timestamp":1618014910900,"user_tz":360,"elapsed":619,"user":{"displayName":"Hailee K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-K-gnA0SyJcEz2RHEd55BjXelge052dstnK98aA=s64","userId":"07223577005008820598"}},"outputId":"8789ed5f-f08c-4630-87df-a2fa0f0eeb42"},"source":["poly_linear_base(X_train, y_train, X_test, y_test)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["standard logic regression in 0.0770 seconds with accuracy of 0.9666666666666667\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lC2794sbG4tP"},"source":["Gaussian Kernel Implementation and time:\n"]},{"cell_type":"code","metadata":{"id":"ZCj2EucC7g9-","executionInfo":{"status":"ok","timestamp":1618015187444,"user_tz":360,"elapsed":601,"user":{"displayName":"Hailee K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-K-gnA0SyJcEz2RHEd55BjXelge052dstnK98aA=s64","userId":"07223577005008820598"}}},"source":["def kernel(Xtrain, ytrain, Xtest, ytest):\n","    tic = time.perf_counter()\n","\n","    train_x = Xtrain\n","    train_y = ytrain\n","    test_x = Xtest\n","    test_y = ytest\n","\n","    kernel = 1.0 * RBF(1.0)\n","    pc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(train_x, train_y)\n","\n","    ac = accuracy_score(test_y, pc.predict(test_x))\n","\n","    toc = time.perf_counter()\n","    print(f\"Classificiation with kernel process in {toc - tic:0.4f} seconds with accuracy of {ac}\")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rGauTnbrIVPI","executionInfo":{"status":"ok","timestamp":1618015189125,"user_tz":360,"elapsed":937,"user":{"displayName":"Hailee K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-K-gnA0SyJcEz2RHEd55BjXelge052dstnK98aA=s64","userId":"07223577005008820598"}},"outputId":"46cb41e7-d263-4ec7-db97-7ee3345ba532"},"source":["kernel(X_train, y_train, X_test, y_test)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Classificiation with kernel process in 0.4373 seconds with accuracy of 0.9333333333333333\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NlGNF6ULLAnC"},"source":["Notice how the kernel in this small data set does not preform as quickly but if we use a larger dataset then our kernel classifier will end up preforming faster than the generic poly feature logistic regression. Again this occurs because for each feature point we need to map it to space and that space can be infinite. If it is then doing the transformation ends up taking alot more time than using a kernel. The kernel allows the data to be mapped into latent space without having to transform its vectors."]},{"cell_type":"code","metadata":{"id":"bys0j2xPOwD3"},"source":[""],"execution_count":null,"outputs":[]}]}
